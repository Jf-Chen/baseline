#### 做法一
```
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 
# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all .unsqueeze(dim=1) #[way,1]
# support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]
support_weight = support_sum / support_sum_all_sq


support_set_proto = (support_weight.unsqueeze(dim=2)).mul( support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 先不算，看看成果
query_pool = query_set.sum(dim=1)
sim = torch.mm(query_pool,proto_pool.permute(1,0)) # [query,way] 
Similarity_list.append(sim)
```

这种做法得到的weight比较小，在0.08左右

结果是

```
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=61.70 +- 0.73 (%), loss=1.2934 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=72.12 +- 0.60 (%), loss=0.9885 (@7)
```

#### 做法二

```
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 
# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all .unsqueeze(dim=1) #[way,1]
# support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]


#### 修改这一句,让feature和原来的数值在同一数量级
support_weight = (shot*h*w)*support_sum / support_sum_all_sq


support_set_proto = (support_weight.unsqueeze(dim=2)).mul( support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 先不算，看看成果
query_pool = query_set.sum(dim=1)
sim = torch.mm(query_pool,proto_pool.permute(1,0)) # [query,way] 
Similarity_list.append(sim)
```

这样的weight在1左右

结果是

```
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=52.49 +- 0.73 (%), loss=1.8772 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=67.89 +- 0.62 (%), loss=0.8779 (@7)
```

#### 分析

我认为是调整了support但是没调整query的原因

#### 做法三

只调整query，按照query和自身的相似度调整

```
# ================ 计算query的自相关，调整权重 =====================#
query_inner_matrix = torch.bmm(query_set,query_set.permute(0,2,1))#[q_num,h*w,h*w]
query_sum = query_inner_matrix.sum(dim=2) #[q_num,h*w]
query_sum_all = query_sum.sum(dim=1)
query_sum_all_sq = query_sum_all.unsqueeze(dim=1)#[q_num,1]
query_weight = (h*w)*query_sum/query_sum_all_sq #[q_num,h*w]

query_after_weight = torch.mul(query_weight.unsqueeze(dim=2),query_set)
query_pool = query_after_weight.sum(dim=1) #[q_num,c]

proto_pool = support_set.sum(dim=1)/shot # [way,c]
sim = torch.mm(query_pool,proto_pool.permute(1,0)) # [query,way] 
Similarity_list.append(sim)
```

结果

```
epoch 1, train 2.5329|0.7810, tval 0.8722|0.6925, val 0.9235|0.6619, 1.5m 1.5m/1.5m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=54.15 +- 0.71 (%), loss=1.8518 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=69.19 +- 0.60 (%), loss=0.8626 (@7)
```

#### 做法四

调整support，以support为标的调整query

```
# ================ 以support为基准，调整权重 =====================#
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 
# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all.unsqueeze(dim=1) #[way,1]
support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]

support_set_proto = torch.mul(support_weight.unsqueeze(dim=2) ,support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 以support 为基准
# 要得到[q_num,way,h*w,shot*h*w]
query_sq = query_set.unsqueeze(dim=1).expand(-1,way,-1,-1).contiguous().view(q_num*way,h*w,c)
support_sq =  support_set.unsqueeze(dim=0).expand(q_num,-1,-1,-1).contiguous().view(q_num*way,shot*h*w,c)
inter_matrix = torch.bmm(query_sq,support_sq.permute(0,2,1)) # [q_num*way,h*w,shot*h*w]
inter_sum =  inter_matrix.sum(dim= 2) # [q_num*way,h*w]
inter_sum_all = inter_sum.sum(dim=1).unsqueeze(dim=1) # [q_num*way,1]
query_weight = (h*w)* inter_sum / inter_sum_all # [q_num*way,h*w]
query_weight_view = query_weight.contiguous().view(q_num,way,h*w)
sim = torch.zeros(q_num,way).cuda()


for j in range(way):
	query_way_weight =  query_weight_view[:,j,:].contiguous().view(q_num,h*w)
	query_way = torch.mul(query_set.contiguous().view(q_num*h*w,c),
					query_way_weight.contiguous().view(q_num*h*w).unsqueeze(dim=1)
					).contiguous().view(q_num,h*w,c) #[q_num,h*w,c]
	query_way_pool = query_way.sum(dim=1) #[q_num,c]
	proto_way = proto_pool[j,:] #[c]
	sim_way = torch.mm(query_way_pool,proto_way.unsqueeze(dim=1)).view(q_num) # [q_num]
	sim[:,j]= sim_way

Similarity_list.append(sim)
```

结果

```
epoch 1, train 2.5967|0.7510, tval 0.9491|0.6518, val 0.9244|0.6521, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=51.30 +- 0.71 (%), loss=1.9064 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=65.41 +- 0.62 (%), loss=0.9447 (@7)
```



#### 做法五

效仿DBRN，给query的权重加超参和正则化。和DBRN不同的是，我不仅给query计算了权重，还给prototype算了权重

```


# ================ 以support为基准，调整权重 =====================#
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 



# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all.unsqueeze(dim=1) #[way,1]
support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]

# 超参
support_weight =  torch.pow(support_weight,neighbor_k)


#### 正则化
support_weight_norm = torch.norm(support_weight,p=2,dim=1,keepdim=True)
support_weight = support_weight/support_weight_norm

support_set_proto = torch.mul(support_weight.unsqueeze(dim=2) ,support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 以support 为基准
# 要得到[q_num,way,h*w,shot*h*w]
query_sq = query_set.unsqueeze(dim=1).expand(-1,way,-1,-1).contiguous().view(q_num*way,h*w,c)
support_sq =  support_set.unsqueeze(dim=0).expand(q_num,-1,-1,-1).contiguous().view(q_num*way,shot*h*w,c)
inter_matrix = torch.bmm(query_sq,support_sq.permute(0,2,1)) # [q_num*way,h*w,shot*h*w]





inter_sum =  inter_matrix.sum(dim= 2) # [q_num*way,h*w]
inter_sum_all = inter_sum.sum(dim=1).unsqueeze(dim=1) # [q_num*way,1]
query_weight = (h*w)* inter_sum / inter_sum_all # [q_num*way,h*w]
query_weight_view = query_weight.contiguous().view(q_num,way,h*w)

# 超参
query_weight_view =  torch.pow(query_weight_view,neighbor_k)

# 正则化
query_weight_norm = torch.norm(query_weight_view,p=2,dim=2,keepdim=True)
query_weight_view = query_weight_view/query_weight_norm

sim = torch.zeros(q_num,way).cuda()


for j in range(way):
	query_way_weight =  query_weight_view[:,j,:].contiguous().view(q_num,h*w)
	query_way = torch.mul(query_set.contiguous().view(q_num*h*w,c),
					query_way_weight.contiguous().view(q_num*h*w).unsqueeze(dim=1)
					).contiguous().view(q_num,h*w,c) #[q_num,h*w,c]
	query_way_pool = query_way.sum(dim=1) #[q_num,c]
	proto_way = proto_pool[j,:] #[c]
	sim_way = torch.mm(query_way_pool,proto_way.unsqueeze(dim=1)).view(q_num) # [q_num]
	sim[:,j]= sim_way

Similarity_list.append(sim)
```

- 其中，neighbor_k = 0.1。步骤是先求和，再超参乘方，再Norm（如果先乘方再求和，再正则化，会出现NaN），

结果

```
epoch 1, train 0.4385|0.9523, tval 0.7407|0.7523, val 0.7343|0.7657, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=63.33 +- 0.71 (%), loss=nan (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=75.07 +- 0.58 (%), loss=0.7334 (@7)
```

- 和只用proto+cos，在epoch=1情况下进行对比

```
epoch 1, train 0.2879|0.9655, tval 0.6292|0.7943, val 0.6204|0.8029, 1.5m 1.5m/1.5m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=63.99 +- 0.69 (%), loss=0.9018 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=79.90 +- 0.54 (%), loss=0.6206 (@7)
```

结论，我设计的方法不仅出现了nan，5-way-5-shot下还大幅降低了

- 改为超参乘方-》求和-》norm的步骤，超参由0.1改为2（0.1出现NaN）

结果

```
epoch 1, train 0.4300|0.9526, tval 0.7320|0.7527, val 0.7222|0.7699, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=63.36 +- 0.70 (%), loss=1.1167 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=75.36 +- 0.58 (%), loss=0.7244 (@7)
```

结论：还是不如proto。但是和 neighbor_k = 0.1。步骤是先求和，再超参乘方，再Norm 相比，准确率有所提升

- 将neighbor改为1.5，看看趋势。结果：出现NaN

- 将neighbor改为3，看看趋势。

结果

```
epoch 1, train 0.4268|0.9521, tval 0.7296|0.7547, val 0.7184|0.7702, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=63.28 +- 0.70 (%), loss=1.1199 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=75.54 +- 0.58 (%), loss=0.7218 (@7)
```

结论：没什么变化，可能是次方高了之后趋于1的原因

- 将neighbor改为10

```
epoch 1, train 0.3639|0.9522, tval 0.6950|0.7596, val 0.6811|0.7714, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=62.84 +- 0.68 (%), loss=1.2303 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=76.12 +- 0.58 (%), loss=0.6859 (@7)
```



- 假如上个实验不理想，考虑借鉴DCAP的做法，将加权 的 原版的prototype相结合，最简单的就是相加。

neighbor_k=2, torch.cat 结果

```
epoch 1, train 0.3916|0.9537, tval 0.7110|0.7576, val 0.6994|0.7729, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=63.47 +- 0.71 (%), loss=1.1212 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=75.73 +- 0.58 (%), loss=0.7031 (@7)
```

neighbor_k=2, (A+B)/2结果

```
epoch 1, train 0.8895|0.9385, tval 1.0089|0.7165, val 1.0251|0.7378, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=61.70 +- 0.73 (%), loss=0.9536 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=71.98 +- 0.61 (%), loss=1.0031 (@7)
```

neighbor_k改成1，（A+B）/2结果

```
epoch 1, train 0.8928|0.9387, tval 1.0122|0.7154, val 1.0288|0.7356, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=61.68 +- 0.73 (%), loss=0.9536 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=71.83 +- 0.61 (%), loss=1.0064 (@7)
```

看来怎么调整超参，调整结合方式，影响都不大

#### 做法六

效仿DCAP，将local和avgpool桥接，再计算权重。


```

```

#### 做法七

鉴于DCAP在dense pretrain之后，使用了attentive pooling。我直接替换成fcanet作为pooling。

先用1个epoch试了下，proto+cos+fcanet

```
epoch 1, train 0.2856|0.9637, tval 0.6262|0.7959, val 0.6234|0.7999, 1.5m 1.5m/1.5m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.5M
test epoch 1: acc=63.99 +- 0.71 (%), loss=0.9026 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.5M
test epoch 1: acc=79.78 +- 0.54 (%), loss=0.6180 (@7)
```

2个epoch 结果

```
epoch 1, train 0.2263|0.9732, tval 0.6202|0.7984, val 0.6019|0.8131, 2.8m 2.8m/5.6m (@5)
epoch 2, train 0.1712|0.9798, tval 0.6144|0.7953, val 0.5820|0.8168, 2.8m 5.6m/5.6m (@5)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.5M
  0% 0/200 [00:00<?, ?it/s]layer is used
test epoch 1: acc=64.16 +- 0.71 (%), loss=0.8983 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.5M
  0% 0/200 [00:00<?, ?it/s]layer is used
test epoch 1: acc=79.94 +- 0.53 (%), loss=0.6067 (@7)
```

结论： 很难说是finetune的效果还是fcanet的效果。而且fcanet没有经过lr从0.1-》0.001的过程，最好冻结几个阶段。

#### 做法八

重新试试DBRN的做法，只修改query，不修改support.



