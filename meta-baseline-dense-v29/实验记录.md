#### 做法一
```
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 
# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all .unsqueeze(dim=1) #[way,1]
# support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]
support_weight = support_sum / support_sum_all_sq


support_set_proto = (support_weight.unsqueeze(dim=2)).mul( support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 先不算，看看成果
query_pool = query_set.sum(dim=1)
sim = torch.mm(query_pool,proto_pool.permute(1,0)) # [query,way] 
Similarity_list.append(sim)
```

这种做法得到的weight比较小，在0.08左右

结果是

```
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=61.70 +- 0.73 (%), loss=1.2934 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=72.12 +- 0.60 (%), loss=0.9885 (@7)
```

#### 做法二

```
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 
# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all .unsqueeze(dim=1) #[way,1]
# support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]


#### 修改这一句,让feature和原来的数值在同一数量级
support_weight = (shot*h*w)*support_sum / support_sum_all_sq


support_set_proto = (support_weight.unsqueeze(dim=2)).mul( support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 先不算，看看成果
query_pool = query_set.sum(dim=1)
sim = torch.mm(query_pool,proto_pool.permute(1,0)) # [query,way] 
Similarity_list.append(sim)
```

这样的weight在1左右

结果是

```
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=52.49 +- 0.73 (%), loss=1.8772 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=67.89 +- 0.62 (%), loss=0.8779 (@7)
```

#### 分析

我认为是调整了support但是没调整query的原因

#### 做法三

只调整query，按照query和自身的相似度调整

```
# ================ 计算query的自相关，调整权重 =====================#
query_inner_matrix = torch.bmm(query_set,query_set.permute(0,2,1))#[q_num,h*w,h*w]
query_sum = query_inner_matrix.sum(dim=2) #[q_num,h*w]
query_sum_all = query_sum.sum(dim=1)
query_sum_all_sq = query_sum_all.unsqueeze(dim=1)#[q_num,1]
query_weight = (h*w)*query_sum/query_sum_all_sq #[q_num,h*w]

query_after_weight = torch.mul(query_weight.unsqueeze(dim=2),query_set)
query_pool = query_after_weight.sum(dim=1) #[q_num,c]

proto_pool = support_set.sum(dim=1)/shot # [way,c]
sim = torch.mm(query_pool,proto_pool.permute(1,0)) # [query,way] 
Similarity_list.append(sim)
```

结果

```
epoch 1, train 2.5329|0.7810, tval 0.8722|0.6925, val 0.9235|0.6619, 1.5m 1.5m/1.5m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=54.15 +- 0.71 (%), loss=1.8518 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=69.19 +- 0.60 (%), loss=0.8626 (@7)
```

#### 做法四

调整support，以support为标的调整query

```
# ================ 以support为基准，调整权重 =====================#
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 
# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all.unsqueeze(dim=1) #[way,1]
support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]

support_set_proto = torch.mul(support_weight.unsqueeze(dim=2) ,support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 以support 为基准
# 要得到[q_num,way,h*w,shot*h*w]
query_sq = query_set.unsqueeze(dim=1).expand(-1,way,-1,-1).contiguous().view(q_num*way,h*w,c)
support_sq =  support_set.unsqueeze(dim=0).expand(q_num,-1,-1,-1).contiguous().view(q_num*way,shot*h*w,c)
inter_matrix = torch.bmm(query_sq,support_sq.permute(0,2,1)) # [q_num*way,h*w,shot*h*w]
inter_sum =  inter_matrix.sum(dim= 2) # [q_num*way,h*w]
inter_sum_all = inter_sum.sum(dim=1).unsqueeze(dim=1) # [q_num*way,1]
query_weight = (h*w)* inter_sum / inter_sum_all # [q_num*way,h*w]
query_weight_view = query_weight.contiguous().view(q_num,way,h*w)
sim = torch.zeros(q_num,way).cuda()


for j in range(way):
	query_way_weight =  query_weight_view[:,j,:].contiguous().view(q_num,h*w)
	query_way = torch.mul(query_set.contiguous().view(q_num*h*w,c),
					query_way_weight.contiguous().view(q_num*h*w).unsqueeze(dim=1)
					).contiguous().view(q_num,h*w,c) #[q_num,h*w,c]
	query_way_pool = query_way.sum(dim=1) #[q_num,c]
	proto_way = proto_pool[j,:] #[c]
	sim_way = torch.mm(query_way_pool,proto_way.unsqueeze(dim=1)).view(q_num) # [q_num]
	sim[:,j]= sim_way

Similarity_list.append(sim)
```

结果

```
epoch 1, train 2.5967|0.7510, tval 0.9491|0.6518, val 0.9244|0.6521, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=51.30 +- 0.71 (%), loss=1.9064 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=65.41 +- 0.62 (%), loss=0.9447 (@7)
```



#### 做法五

效仿DBRN，给query的权重加超参和正则化。和DBRN不同的是，我不仅给query计算了权重，还给prototype算了权重

```


# ================ 以support为基准，调整权重 =====================#
# ================ 计算support set 自身权重和query到 support set的权重 =====================#
# 只有这个bmm作为相似度，让人疑惑
support_inner_matrix = torch.bmm(support_set,support_set.permute(0,2,1)) 



# [way,shot*h*w,shot*h*w]
support_sum = support_inner_matrix.sum(dim=2) #[way,shot*h*w]
support_sum_all = support_sum.sum(dim=1) + 1e-08# [way]
support_sum_all_sq= support_sum_all.unsqueeze(dim=1) #[way,1]
support_weight = (shot*h*w)*support_sum / support_sum_all_sq #[way,shot*h*w]

# 超参
support_weight =  torch.pow(support_weight,neighbor_k)


#### 正则化
support_weight_norm = torch.norm(support_weight,p=2,dim=1,keepdim=True)
support_weight = support_weight/support_weight_norm

support_set_proto = torch.mul(support_weight.unsqueeze(dim=2) ,support_set) # [way,shot*h*w,c]
proto_pool = support_set_proto.sum(dim=1)/shot # [way,c]

# 计算query
# 以support 为基准
# 要得到[q_num,way,h*w,shot*h*w]
query_sq = query_set.unsqueeze(dim=1).expand(-1,way,-1,-1).contiguous().view(q_num*way,h*w,c)
support_sq =  support_set.unsqueeze(dim=0).expand(q_num,-1,-1,-1).contiguous().view(q_num*way,shot*h*w,c)
inter_matrix = torch.bmm(query_sq,support_sq.permute(0,2,1)) # [q_num*way,h*w,shot*h*w]





inter_sum =  inter_matrix.sum(dim= 2) # [q_num*way,h*w]
inter_sum_all = inter_sum.sum(dim=1).unsqueeze(dim=1) # [q_num*way,1]
query_weight = (h*w)* inter_sum / inter_sum_all # [q_num*way,h*w]
query_weight_view = query_weight.contiguous().view(q_num,way,h*w)

# 超参
query_weight_view =  torch.pow(query_weight_view,neighbor_k)

# 正则化
query_weight_norm = torch.norm(query_weight_view,p=2,dim=2,keepdim=True)
query_weight_view = query_weight_view/query_weight_norm

sim = torch.zeros(q_num,way).cuda()


for j in range(way):
	query_way_weight =  query_weight_view[:,j,:].contiguous().view(q_num,h*w)
	query_way = torch.mul(query_set.contiguous().view(q_num*h*w,c),
					query_way_weight.contiguous().view(q_num*h*w).unsqueeze(dim=1)
					).contiguous().view(q_num,h*w,c) #[q_num,h*w,c]
	query_way_pool = query_way.sum(dim=1) #[q_num,c]
	proto_way = proto_pool[j,:] #[c]
	sim_way = torch.mm(query_way_pool,proto_way.unsqueeze(dim=1)).view(q_num) # [q_num]
	sim[:,j]= sim_way

Similarity_list.append(sim)
```

其中，neighbor_k = 0.1。步骤是先求和，再超参乘方，再Norm（如果先乘方再求和，再正则化，会出现NaN），

结果

```
epoch 1, train 0.4385|0.9523, tval 0.7407|0.7523, val 0.7343|0.7657, 1.6m 1.6m/1.6m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=63.33 +- 0.71 (%), loss=nan (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=75.07 +- 0.58 (%), loss=0.7334 (@7)
```

和只用proto+cos，在epoch=1情况下进行对比

```
epoch 1, train 0.2879|0.9655, tval 0.6292|0.7943, val 0.6204|0.8029, 1.5m 1.5m/1.5m (@0)
current_time== 0Y-0M-0D-0H-0Minu
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=63.99 +- 0.69 (%), loss=0.9018 (@7)
set gpu: 0
dataset: torch.Size([3, 80, 80]) (x12000), 20
num params: 12.4M
test epoch 1: acc=79.90 +- 0.54 (%), loss=0.6206 (@7)
```

结论，我设计的方法不仅出现了nan，5-way-5-shot下还大幅降低了

#### 做法六

效仿DCAP，将local和avgpool桥接，再计算权重。

