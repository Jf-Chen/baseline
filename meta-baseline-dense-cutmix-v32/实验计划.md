finetune失败的原因应该是gap的问题，但是直接用local更加失败。

应该试一试空间注意力。



DCAP使用可训练的liear来给hw个local加权，来替代简单的gap，形成[c]的embedding，我认为这相当于空间注意力。并且这没有使用到taks special（包括类间和类内）的信息。



类内信息可以是自注意力，类间信息可以是交互注意力



#### 实验一 v32

实验Learning to focus: cascaded feature matching network for few-shot image recognition 方法

用support补充进入query



然后为了保持整体，加权query最后要加上原图

由于support图片数量稀少，取support的均值不妥。用每张图都做一遍attention

#### 实验一 v32

因为 A Closer Look at Few-shot Classification 2019认可元学习在微调阶段的作用，声称微调的关键在于减少类内差距。closer本身固定了预训练得到的网络。

1. 试试重新校正原型，减少离散点的影响，是否对5-shot起作用
2. 在fine tuning阶段加入cutmix



#### 实验二 v32

受Few-Shot Object Detection on Remote Sensing Images via
Shared Attention Module and Balanced Fine-Tuning Strategy启发，加入共享空间注意力

把CBAM的空间注意力先直接加入resnet12中，meta-train时冻结encoder的前四层

对比 同样冻结resnet 4 layer的dense classifie的分类效果，得出空间注意力作用的结论

shared self attention

